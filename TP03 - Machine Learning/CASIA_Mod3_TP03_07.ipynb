{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "121e3552-62f5-4f52-9e3f-2117ec519dd6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 1"
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# ============================================================\n",
    "# 1. Imports MLflow\n",
    "# ============================================================\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# ============================================================\n",
    "# 2. Chemin du modèle MLflow (adapter si différent)\n",
    "# ============================================================\n",
    "\n",
    "model_uri = \"models:/SpaceshipRF/1\"   # version 1 du modèle enregistré\n",
    "# Si tu veux toujours la dernière version :\n",
    "# model_uri = \"models:/SpaceshipRF/Production\"\n",
    "\n",
    "# ============================================================\n",
    "# 3. Charger le modèle depuis MLflow\n",
    "# ============================================================\n",
    "\n",
    "loaded_model = mlflow.sklearn.load_model(model_uri)\n",
    "print(\"Modèle MLflow chargé avec succès !\")\n",
    "\n",
    "# ============================================================\n",
    "# 4. Charger les données de test (Spark → Pandas)\n",
    "# ============================================================\n",
    "\n",
    "test_df_spark = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"/Volumes/ngow_lakehouse/ml_sandbox/data/test.csv\")\n",
    "\n",
    "# IMPORTANT : même nettoyage que train !\n",
    "cols_to_remove = [\"PassengerId\", \"Cabin\", \"Name\"]\n",
    "for col in cols_to_remove:\n",
    "    if col in test_df_spark.columns:\n",
    "        test_df_spark = test_df_spark.drop(col)\n",
    "\n",
    "test_pd = test_df_spark.toPandas()\n",
    "\n",
    "# ============================================================\n",
    "# 5. Faire les prédictions avec le modèle MLflow\n",
    "# ============================================================\n",
    "\n",
    "predictions = loaded_model.predict(test_pd)\n",
    "\n",
    "# ============================================================\n",
    "# 6. Ajouter les prédictions au DataFrame de test\n",
    "# ============================================================\n",
    "\n",
    "test_pd[\"Predicted_HomePlanet\"] = predictions\n",
    "\n",
    "# Afficher les premières lignes\n",
    "test_pd.head()\n",
    "\n",
    "# ============================================================\n",
    "# 7. (OPTIONNEL) Sauvegarder les prédictions dans un CSV\n",
    "# ============================================================\n",
    "\n",
    "output_path = \"/Volumes/ngow_lakehouse/ml_sandbox/data/output/predictions.csv\"\n",
    "dbutils.fs.mkdirs(\"/Volumes/ngow_lakehouse/ml_sandbox/data/output\")\n",
    "test_pd.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"Prédictions sauvegardées dans :\", output_path)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "CASIA_Mod3_TP03_07",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
