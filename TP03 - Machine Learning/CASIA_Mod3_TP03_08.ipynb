{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63a8bbd6-5252-4f1f-89ba-b5b00889dd89",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install optuna and xgboost"
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "%pip install optuna xgboost==2.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "624a4f80-8b96-4aef-b191-948b2ded291a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 1"
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# ============================================================\n",
    "# XGBoost optimisé (scikit-learn API + Optuna)\n",
    "# ============================================================\n",
    "\n",
    "# (0) Assurer la dispo d'XGBoost sur le cluster (une seule fois si besoin)\n",
    "# %pip install xgboost==2.0.3\n",
    "\n",
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# (1) Charger & nettoyer\n",
    "train_df_spark = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\").option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"/Volumes/ngow_lakehouse/ml_sandbox/data/train.csv\")\n",
    "\n",
    "for col in [\"PassengerId\", \"Cabin\", \"Name\"]:\n",
    "    if col in train_df_spark.columns:\n",
    "        train_df_spark = train_df_spark.drop(col)\n",
    "\n",
    "target_col = \"HomePlanet\"\n",
    "train_pd = train_df_spark.toPandas()\n",
    "train_pd = train_pd.dropna(subset=[target_col])\n",
    "\n",
    "X = train_pd.drop(columns=[target_col])\n",
    "y = train_pd[target_col]\n",
    "\n",
    "numeric_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "categorical_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "# (2) Préprocesseurs\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\"))\n",
    "])\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_cols),\n",
    "        (\"cat\", categorical_transformer, categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# (3) Fonction objectif Optuna\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 200, 1200),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 1e-1, log=True),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 1.0, log=True),\n",
    "        \"objective\": \"multi:softmax\",\n",
    "        \"eval_metric\": \"mlogloss\",\n",
    "        \"random_state\": 42,\n",
    "        \"n_jobs\": -1,\n",
    "        \"tree_method\": \"hist\"  # rapide et adapté CPU\n",
    "    }\n",
    "\n",
    "    xgb = XGBClassifier(**params)\n",
    "\n",
    "    clf = Pipeline(steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"model\", xgb)\n",
    "    ])\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(clf, X, y, cv=cv, scoring=\"accuracy\", n_jobs=-1)\n",
    "    return scores.mean()\n",
    "\n",
    "# (4) Optimisation\n",
    "study_xgb = optuna.create_study(direction=\"maximize\")\n",
    "study_xgb.optimize(objective, n_trials=40, show_progress_bar=True)\n",
    "\n",
    "print(\"Meilleurs hyperparamètres XGB :\", study_xgb.best_params)\n",
    "print(\"Meilleure accuracy CV XGB     :\", round(study_xgb.best_value, 4))\n",
    "\n",
    "# (5) Entraînement final + holdout\n",
    "best_xgb = XGBClassifier(**{**study_xgb.best_params, \"random_state\":42, \"n_jobs\":-1, \"tree_method\":\"hist\"})\n",
    "clf_xgb = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"model\", best_xgb)])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "clf_xgb.fit(X_train, y_train)\n",
    "y_pred = clf_xgb.predict(X_test)\n",
    "\n",
    "print(\"Accuracy XGB holdout :\", round(accuracy_score(y_test, y_pred), 4))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# (6) Importance des features (gain-based)\n",
    "# Récupération des noms après OHE\n",
    "ohe = clf_xgb.named_steps[\"preprocessor\"].named_transformers_[\"cat\"].named_steps[\"onehot\"]\n",
    "ohe_feature_names = ohe.get_feature_names_out(categorical_cols)\n",
    "feature_names = np.concatenate([numeric_cols, ohe_feature_names])\n",
    "\n",
    "# importance depuis booster\n",
    "booster = clf_xgb.named_steps[\"model\"].get_booster()\n",
    "score_dict = booster.get_score(importance_type=\"gain\")\n",
    "\n",
    "# mapper f0..fn -> noms colonnes\n",
    "# XGBoost mappe les colonnes dans l'ordre du vectoriseur (preprocessor)\n",
    "# on construit un tableau importance aligné\n",
    "importances = np.zeros(len(feature_names))\n",
    "for k, v in score_dict.items():\n",
    "    # k est de la forme \"f123\"\n",
    "    idx = int(k[1:])\n",
    "    if idx < len(importances):\n",
    "        importances[idx] = v\n",
    "\n",
    "fi_xgb = pd.DataFrame({\"feature\": feature_names, \"importance_gain\": importances}) \\\n",
    "            .sort_values(\"importance_gain\", ascending=False)\n",
    "print(fi_xgb.head(20))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "CASIA_Mod3_TP03_08",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
