{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee2c02dd-20e0-438f-9e2f-bb346e63d9f5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install Dependencies"
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "%pip install \"optuna>=3.4\" scikit-learn lightgbm optuna-dashboard psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "619635e7-59e6-4dd0-865c-73286838b58b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install Dependencies"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9436727c-9ef3-4e0d-8a0d-4e1267cbed77",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 3"
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "%pip install \"optuna>=3.4\" scikit-learn lightgbm optuna-dashboard psycopg2-binary\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Optuna HPO Distribué — Script unique\n",
    "\n",
    "USAGE RAPIDE\n",
    "============\n",
    "1) Démo locale (1 process) :\n",
    "   python optuna_dist_hpo.py --n-trials 50\n",
    "\n",
    "2) Démo locale multi-process (8 workers / même machine) :\n",
    "   python optuna_dist_hpo.py --role spawn --num-workers 8 --n-trials 50\n",
    "\n",
    "3) Exécution distribuée (plusieurs machines/nœuds/pods pointant vers le même storage) :\n",
    "   # Sur chaque machine (ou job), utilisez la même URL de storage + même study :\n",
    "   export OPTUNA_STORAGE=\"postgresql+psycopg2://user:pwd@db-host:5432/optuna_db\"\n",
    "   export OPTUNA_STUDY=\"mon_study_hpo\"\n",
    "   python optuna_dist_hpo.py --role worker --n-trials 50\n",
    "\n",
    "DÉPENDANCES CONSEILLÉES (pour l'objectif ML)\n",
    "============================================\n",
    "pip install \"optuna>=3.4\" scikit-learn lightgbm optuna-dashboard psycopg2-binary\n",
    "\n",
    "NOTES\n",
    "=====\n",
    "- Par défaut, le script utilise SQLite (fichier local \"optuna.db\") pour la démo.\n",
    "  Pour **vrai distribué**, utilisez PostgreSQL/MySQL (OPTUNA_STORAGE).\n",
    "- Le sampler TPE est configuré en mode avancé (multivarié + constant liar).\n",
    "- Le pruner Successive Halving est activé (report à chaque fold/étape).\n",
    "- Les meilleurs paramètres/valeurs sont sauvegardés dans :\n",
    "  - best_params.json\n",
    "  - best_value.txt\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import argparse\n",
    "import json\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# -------- Détection des libs ML (optionnelles) --------\n",
    "_HAS_SK = False\n",
    "_HAS_LGBM = False\n",
    "try:\n",
    "    import numpy as np\n",
    "    _HAS_NUMPY = True\n",
    "except Exception:\n",
    "    _HAS_NUMPY = False\n",
    "\n",
    "try:\n",
    "    import optuna\n",
    "    from optuna.samplers import TPESampler\n",
    "    from optuna.pruners import SuccessiveHalvingPruner\n",
    "except Exception as e:\n",
    "    print(\"[ERREUR] Optuna est requis. Installez-le : pip install optuna\", file=sys.stderr)\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    from sklearn.datasets import load_breast_cancer\n",
    "    from sklearn.model_selection import KFold\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    _HAS_SK = True\n",
    "except Exception:\n",
    "    _HAS_SK = False\n",
    "\n",
    "try:\n",
    "    from lightgbm import LGBMClassifier\n",
    "    _HAS_LGBM = True\n",
    "except Exception:\n",
    "    _HAS_LGBM = False\n",
    "\n",
    "\n",
    "# -------- Config & utils --------\n",
    "@dataclass\n",
    "class Config:\n",
    "    study_name: str\n",
    "    storage_url: str\n",
    "    direction: str\n",
    "    n_trials: int\n",
    "    role: str           # \"worker\" | \"spawn\" | \"single\"\n",
    "    num_workers: int\n",
    "    objective_kind: str # \"auto\" | \"ml\" | \"ackley\"\n",
    "    seed: int\n",
    "\n",
    "\n",
    "def build_storage(storage_url: str):\n",
    "    \"\"\"Retourne un storage Optuna.\n",
    "    - Si URL RDB (postgresql/mysql), on utilise RDBStorage.\n",
    "    - Sinon SQLite local pour démo.\"\"\"\n",
    "    if storage_url.startswith((\"postgresql\", \"mysql\")):\n",
    "        return optuna.storages.RDBStorage(url=storage_url)\n",
    "    # Fallback: SQLite\n",
    "    return optuna.storages.RDBStorage(url=storage_url)\n",
    "\n",
    "\n",
    "def build_study(cfg: Config):\n",
    "    sampler = TPESampler(\n",
    "        multivariate=True,\n",
    "        group=True,\n",
    "        constant_liar=True,   # important pour forte parallélisation\n",
    "        n_startup_trials=20,\n",
    "        seed=cfg.seed,\n",
    "    )\n",
    "    pruner = SuccessiveHalvingPruner(min_resource=1, reduction_factor=3)\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        storage=build_storage(cfg.storage_url),\n",
    "        study_name=cfg.study_name,\n",
    "        direction=cfg.direction,\n",
    "        load_if_exists=True,\n",
    "        sampler=sampler,\n",
    "        pruner=pruner,\n",
    "    )\n",
    "    return study\n",
    "\n",
    "\n",
    "# -------- Objectifs --------\n",
    "def objective_ml(trial: optuna.trial.Trial) -> float:\n",
    "    \"\"\"Objectif ML : LightGBM + KFold sur dataset breast_cancer (sans accès réseau).\"\"\"\n",
    "    if not (_HAS_SK and _HAS_LGBM):\n",
    "        raise RuntimeError(\"Objectif ML indisponible : installez scikit-learn et lightgbm, \"\n",
    "                           \"ou utilisez --objective ackley (fallback sans dépendances).\")\n",
    "\n",
    "    # Espace de recherche avancé\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 15, 255),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 200),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 1e-1, log=True),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 1e-1, log=True),\n",
    "        \"random_state\": 42,\n",
    "        \"n_jobs\": 1,  # un essai = 1 worker CPU\n",
    "    }\n",
    "\n",
    "    data = load_breast_cancer()\n",
    "    X = data.data\n",
    "    y = data.target\n",
    "\n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "    for fold, (tr_idx, va_idx) in enumerate(kf.split(X)):\n",
    "        # Pipeline: standardisation + LGBM\n",
    "        model = Pipeline([\n",
    "            (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n",
    "            (\"clf\", LGBMClassifier(**params))\n",
    "        ])\n",
    "        model.fit(X[tr_idx], y[tr_idx])\n",
    "        prob = model.predict_proba(X[va_idx])[:, 1]\n",
    "        score = roc_auc_score(y[va_idx], prob)  # AUC\n",
    "        scores.append(score)\n",
    "\n",
    "        # Report et pruning : on minimise 1 - AUC\n",
    "        trial.report(1.0 - float(score), step=fold)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return 1.0 - float(sum(scores) / len(scores))\n",
    "\n",
    "\n",
    "def objective_ackley(trial: optuna.trial.Trial) -> float:\n",
    "    \"\"\"Fallback sans dépendances ML : minimise la fonction d'Ackley (d=5) avec 'epochs' simulés.\n",
    "       Permet le pruning via report à chaque 'epoch'.\"\"\"\n",
    "    if not _HAS_NUMPY:\n",
    "        # Implémentation minimale sans numpy (moins rapide)\n",
    "        import math\n",
    "        d = 5\n",
    "        # Espace de recherche"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "CASIA_Mod3_TP03_09",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
